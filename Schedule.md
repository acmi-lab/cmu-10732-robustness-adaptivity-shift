
| Date | Topic | Readings |Speaker(s) | Weekly Check-in | Notes |
| ---: | :--- | :--- | :--- | :--- | --- |
|  8/29 | **Intro and Logistics** | None | Zack Lipton  | None | None |
|  9/12 | **Label and Covariate Shift**| **Required**: <ul><li>[Detecting and Correcting for Label Shift with Black Box Predictors](https://arxiv.org/abs/1802.03916) </li><li> [Mixture Proportion Estimation and PU Learning: A Modern Approach](https://arxiv.org/abs/2111.00980)  </li></ul> **Optional:** <ul> <li> [Failing Loudly: An Empirical Study of Methods for Detecting Dataset Shift](https://arxiv.org/abs/1810.11953) </li> <li>[What is the Effect of Importance Weighting in Deep Learning?](https://arxiv.org/abs/1812.03372)</li> <li>[Domain Adaptation under Open Set Label Shift](https://arxiv.org/abs/2207.13048)</li><li>[Unsupervised Learning Under Latent Label Shift](https://arxiv.org/abs/2207.13179)</li> </ul> **Background**: <ul> <li>[On Causal and Anticausal Learning](https://icml.cc/2012/papers/625.pdf)</li> <li>[Domain Adaptation under Target and Conditional Shift](http://proceedings.mlr.press/v28/zhang13d.html)</li><li>[Improving predictive inference under covariate shift by weighting the log-likelihood function](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.370.4921&rep=rep1&type=pdf)</li></ul> | Zack Lipton | [Week 1](https://docs.google.com/forms/d/e/1FAIpQLSdjFHenGtWOpNI79icBStYgiN41B9WPGuF-q4pgfMbAGaWJDQ/viewform) <br> ddl: 09/11 11:59PM | [notes](https://github.com/acmi-lab/cmu-10732-robustness-adaptivity-shift/blob/main/notes/1.pdf) |
|  9/19 | **Deep Learning Heuristics**| **Required**: <ul><li>[Domain-Adversarial Training of Neural Networks ](https://arxiv.org/abs/1505.07818)</li><li>[FixMatch](https://arxiv.org/abs/2001.07685)</li><li>[Connect, not collapse](https://arxiv.org/abs/2204.00570)</li> </ul> **Also Covered**: <li>[TENT: Fully Test-Time Adaptation by Entropy Minimization](https://arxiv.org/abs/2006.10726)</li> <li>[On Learning Invariant Representations for Domain Adaptation](http://proceedings.mlr.press/v97/zhao19a/zhao19a.pdf)</li> <li>[Semi-supervised Models are Strong Unsupervised Domain Adaptation Learners](https://arxiv.org/abs/2106.00417)</li> <li>[mixup: Beyond Empirical Risk Minimization](https://arxiv.org/abs/1710.09412)</li> <li>[Improve Unsupervised Domain Adaptation with Mixup Training](https://arxiv.org/abs/2001.00677)</li> <li>[Domain Adaptation with Asymmetrically-Relaxed Distribution Alignment](https://www.cs.cmu.edu/~yw4/files/icml19.pdf)</li> **Auxiliary**: <li>[In Search of Lost Domain Generalization](https://arxiv.org/abs/2007.01434)</li> <li>[Learning to Generalize: Meta-Learning for Domain Generalization](https://arxiv.org/abs/1710.03463)</li> **Background**: <li>[Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks](https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.664.3543)</li> <li>[Regularization With Stochastic Transformations and Perturbations for Deep Semi-Supervised Learning](https://arxiv.org/abs/1606.04586)</li> <li>[Deep Domain Confusion: Maximizing for Domain Invariance](https://arxiv.org/abs/1412.3474)</li>| Xueying Ding, Prince Wang, Manley Roberts  | [Week 2](https://docs.google.com/forms/d/e/1FAIpQLSd5CMFINC3T5vGMsdPyGGGXlf8laF6jzIhnEoGVuBm9DV4mBw/viewform) <br> ddl: 09/19 11:59AM| [notes](https://drive.google.com/file/d/1Aylp1FsROcHK_akoTL9U0nBfdFhUhNgs/view?usp=sharing) |
|  9/26 | **Adaptation under Causally Structured Shifts**| **Required**: <ul><li>[Domain Adaptation by Using Causal Inference to Predict Invariant Conditional Distributions](https://proceedings.neurips.cc/paper/2018/file/39e98420b5e98bfbdc8a619bef7b8f61-Paper.pdf)</li><li>[Domain adaptation under structural causal models](https://arxiv.org/abs/2010.15764)</li></ul> **Background**: <ul><li>[Transportability of causal and statistical relations](https://www.aaai.org/ocs/index.php/AAAI/AAAI11/paper/viewPaper/3769)</li><li>[Causal inference and the data-fusion problem](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=r5U-D7YAAAAJ&citation_for_view=r5U-D7YAAAAJ:vbGhcppDl1QC)</li></ul> **Optional** <ul><li>[General identifiability with arbitrary surrogate experiments](http://proceedings.mlr.press/v115/lee20b.html)</li><li>[Joint Causal Inference from Multiple Contexts](https://www.jmlr.org/papers/volume21/17-123/17-123.pdf)</li><li>[Multi-source domain adaptation: A causal view](https://mingming-gong.github.io/papers/AAAI_MULTI.pdf)</li>[Transporting causal mechanisms for unsupervised domain adaptation](https://arxiv.org/abs/2107.11055)<li>[Domain adaptation with conditional transferable components](http://proceedings.mlr.press/v48/gong16.pdf)</li><li>[Causal generative domain adaptation networks](https://arxiv.org/abs/1804.04333)</li></ul>  | Beomjo Park, Daniel Jeong   | [Week 3](https://docs.google.com/forms/d/e/1FAIpQLSd2pw3b9Wh_ErrvizrJ50dCeAD5zi7EybRxcUhmfbjnnDy4Dw/viewform) <br> ddl: 09/23 1:00PM| [notes](https://drive.google.com/file/d/1RzbsRQoW8uMBBMl6V6hII6iSsXxGJDl2/view?usp=sharing) |
|  10/3 | **Domain Generalization**| **Required**: <ul><li>[Invariant Causal Prediction](https://arxiv.org/abs/1501.01332)</li><li>[Invariant Risk Minimization](https://arxiv.org/abs/1907.02893)</li><li>[In Search of Lost Domain Generalization](https://arxiv.org/abs/2007.01434)</li></ul> **Optional**: <ul><li>[The Risks of Invariant Risk Minimization](https://arxiv.org/abs/2010.05761)</li><li>[Learning to Generalize: Meta-Learning for Domain Generalization](https://arxiv.org/abs/1710.03463)</li></ul>  | Sameer Jain, Pranav Mani   | [Week 4](https://forms.gle/m8NmhQnnXsocj4Lv5) <br> ddl: 10/3 12:00pm | [notes](https://github.com/acmi-lab/cmu-10732-robustness-adaptivity-shift/blob/main/notes/domain_generalization_notes.pdf) |
|  10/10 | **Adversarial Examples**| **Required**: <ul><li>[Explaining and Harnessing Adversarial Examples](https://arxiv.org/abs/1412.6572)</li><li>[Certified Defenses against Adversarial Examples](https://arxiv.org/abs/1801.09344)</li></ul> **Background:** <ul><li>[Intriguing properties of neural networks](https://arxiv.org/abs/1312.6199)</li></ul> **Optional:** <ul><li>[Certified Adversarial Robustness via Randomized Smoothing](https://arxiv.org/abs/1902.02918)</li><li>[Towards deep learning models resistant to adversarial attacks](https://arxiv.org/abs/1706.06083)</li><li>[Robust Physical-World Attacks on Deep Learning Visual Classification](https://arxiv.org/abs/1707.08945)</li><li>[Evaluating Robustness of Neural Networks with Mixed Integer Programming](https://arxiv.org/abs/1711.07356)</li></ul> | Dhananjay Ashok, Olivier Filion | [Week 5](https://forms.gle/i9iN91rjL1MDQxRY7) <br> ddl: 10/10 11:59AM| [notes](https://drive.google.com/file/d/1GEJP8r0cZt3oLUzLTMZoSkP-NevG5zci/view?usp=sharing) |
|  10/17 | **FALL BREAK/NO CLASS**| | | Week 6 | None |
|  10/24 | **Online Learning/Gradual Shift**|**Required**: <ul><li>[Understanding Self-Training for Gradual Domain Adaptation](http://proceedings.mlr.press/v119/kumar20c/kumar20c.pdf)</li><li>[Optimal Dynamic Regret in Exp-Concave Online Learning](https://arxiv.org/pdf/2104.11824.pdf)</li></ul>  **Background:** <ul><li>[Theoretical Analysis of Self-Training with Deep Networks on Unlabeled Data](https://arxiv.org/abs/2010.03622)</li><li>[Understanding Gradual Domain Adaptation: Improved Analysis, Optimal Path and Beyond](https://arxiv.org/abs/2204.08200)</li><li>[Gradual Domain Adaptation without Indexed Intermediate Domains](https://arxiv.org/abs/2207.04587)</li><li>[Continuous manifold based adaptation for evolving visual domains](https://ieeexplore.ieee.org/document/6909511)</li><li>[Non-Stationary Stochastic Optimization](https://www0.gsb.columbia.edu/mygsb/faculty/research/pubfiles/6022/Besbes_Gur_Zeevi_nonstationary.pdf)</li></ul> | External speakers: Yu-Xiang Wang, Ananya Kumar| [Week 7](https://forms.gle/i84LBcCS7ppRofyCA) <br> ddl:10/24 12pm | [notes](https://github.com/acmi-lab/cmu-10732-robustness-adaptivity-shift/blob/main/notes/lecture_notes_dynamic_regret.pdf) |
|  10/31 | **Conformal Inference, Sequential Testing**|**Required:** <ul><li>[Adaptive Conformal Inference Under Distribution Shift](https://arxiv.org/abs/2106.00170)</li><li>[Conformal Prediction Beyond Exchangeability](https://arxiv.org/abs/2202.13415)</li></ul>**Background:**<ul><li>[A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification](https://arxiv.org/abs/2107.07511)</li></ul>**Optional:**<ul><li>[Conformal Prediction Under Covariate Shift](https://arxiv.org/abs/1904.06019)</li><li>[Conformal Inference for Online Prediction with Arbitrary Distribution Shifts](https://arxiv.org/abs/2208.08401)</li><li>[Adaptive Conformal Predictions for Time Series](https://arxiv.org/abs/2202.07282)</li><li>[Testing for Outliers with Conformal p-values](https://arxiv.org/abs/2104.08279)</li><li>[Retrain or not retrain: conformal test martingales for change-point detection](https://proceedings.mlr.press/v152/vovk21b.html)</li></ul>|Aleksandr Podkopaev, Aditya Gangrade| [Week 8](https://forms.gle/gueZpAq6XoNNQKkB9) <br> ddl: 10/31 12pm | None |
|  11/7 | **Feedback-driven Shifts (Recommender Systems)**| **Required:**<ul><li>[Estimating and Penalizing Induced Preference Shifts in Recommender Systems](https://proceedings.mlr.press/v162/carroll22a/carroll22a.pdf)</li><li>[Recommendations as Treatments: Debiasing Learning and Evaluation](http://proceedings.mlr.press/v48/schnabel16.pdf)</li></ul>**Optional:**<ul><li>[Recommendations and User Agency: The Reachability of Collaboratively-Filtered Information](https://dl.acm.org/doi/pdf/10.1145/3351095.3372866)</li><li>[Degenerate Feedback Loops in Recommender Systems](https://dl.acm.org/doi/abs/10.1145/3306618.3314288)</li><li>[Feedback Loop and Bias Amplification in Recommender Systems](https://dl.acm.org/doi/abs/10.1145/3340531.3412152)</li></ul>| Thomson Yen, Mel Andrews | [Week 9](https://forms.gle/wpXfwA2PMMKUznci7) <br> ddl: 11/7 12pm | None |
|  11/14 | **Feedback-driven Shifts (Strategic Classification)**|**Readings:**<ul><li>[Performative Prediction](http://proceedings.mlr.press/v119/perdomo20a/perdomo20a.pdf)</li><li>[Strategic Classification is Causal Modeling in Disguise](https://arxiv.org/abs/1910.10362)</li><li>[How do classifiers induce agents to invest effort strategically?](https://arxiv.org/abs/1807.05307)</li><li>[A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning](https://arxiv.org/abs/1011.0686)</li></ul>| John Long, Michael Agaby| Week 10 | None |
|  11/21 | **Distribution Shifts in the Wild**|**Readings:**<ul><li>[Data Validation for Machine Learning](https://proceedings.mlsys.org/book/2019/file/5878a7ab84fb43402106c575658472fa-Paper.pdf)</li><li>[The Clinician and Dataset Shift in Artificial Intelligence](https://www.nejm.org/doi/full/10.1056/NEJMc2104626)</li><li>[Domain Adaptation for Medical Image Analysis: A Survey](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9011180/)</li><li>[The Effect of Natural Distribution Shift on Question Answering Models](https://proceedings.mlr.press/v119/miller20a.html)</li><li>[Unbiased Look at Dataset Bias](https://people.csail.mit.edu/torralba/publications/datasets_cvpr11.pdf)</li></ul>| Nicholas Chen, Degan Hao| Week 11 | None |
|  11/28 | **Fairness and Distribution Shifts**|**Readings:**<ul><li>[Delayed Impact of Fair Machine Learning](https://arxiv.org/abs/1803.04383)</li><li>[Preventing Fairness Gerrymandering: Auditing and Learning for Subgroup Fairness](http://proceedings.mlr.press/v80/kearns18a.html)</li><li>[Retiring Adult: New Datasets for Fair Machine Learning](https://proceedings.neurips.cc/paper/2021/hash/32e54441e6382a7fbacbbbaf3c450059-Abstract.html)</li></ul> |Lingwei Cheng, Mohsen Ferdosi | Week 12 | None |
|  12/5 | No class for NeurIPS| | | Week 13 | None |
|  TBD | Poster Session| | | Week 14 | None |
